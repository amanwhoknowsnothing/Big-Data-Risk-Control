{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "trusted": true,
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "id": "D89B0EAB542642F98900CCC4D4E49360",
    "runtime": {
     "status": "default",
     "execution_status": null,
     "is_visible": false
    },
    "scrolled": false,
    "notebookId": "6515cdbac6bc713b13747e43"
   },
   "source": [
    "## 集成学习算法  \n",
    "\n",
    "集成学习通过构建多个学习器，并结合多个学习器的结果进行预测，通常可以获得比单一学习器更高的准确性和泛化能力。  \n",
    "运行时选择【**风控专用镜像**】  \n",
    "\n",
    "**1. 随机森林（random forest）**  \n",
    "**2. GBDT（Gradient Boost Decision Tree）**  \n",
    "**3. XGBoost（eXtreme Gradient Boosting）**  \n",
    "**4. LightGBM（Light Gradient Boosting Machine）**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BC480DAFB9184B1DA9C5FF8F7FB9A4DF",
    "notebookId": "6515cdbac6bc713b13747e43",
    "runtime": {
     "status": "default",
     "execution_status": null,
     "is_visible": false
    },
    "scrolled": false,
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "810CF482643D4985AFA558B9F60AF78D",
    "notebookId": "6515cdbac6bc713b13747e43",
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "trusted": true,
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "# 随机森林\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from utils import data_utils\n",
    "\n",
    "# 导入数值型样例数据\n",
    "train_x, test_x, train_y, test_y = data_utils.get_x_y_split(test_rate=0.2)\n",
    "\n",
    "# 训练随机森林模型\n",
    "clf = RandomForestClassifier(n_estimators=200,\n",
    "                             criterion='gini',\n",
    "                             max_depth=6,\n",
    "                             min_samples_leaf=15,\n",
    "                             bootstrap=True,\n",
    "                             oob_score=True,\n",
    "                             random_state=88)\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "auc_score = roc_auc_score(test_y, clf.predict_proba(test_x)[:, 1])\n",
    "print(\"随机森林模型 AUC: \", auc_score)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "随机森林模型 AUC:  0.7455357142857143\n"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n  \"X does not have valid feature names, but\"\n"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1C2CB1EDA6EE4EEE92A306A46C1E425D",
    "notebookId": "6515cdbac6bc713b13747e43",
    "runtime": {
     "status": "default",
     "execution_status": null,
     "is_visible": false
    },
    "scrolled": false,
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. GBDT"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F10370BE44A14C789FE11AA879ED6948",
    "notebookId": "6515cdbac6bc713b13747e43",
    "collapsed": false,
    "scrolled": false,
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "trusted": true
   },
   "source": [
    "# GBDT\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from utils import data_utils\n",
    "\n",
    "# 导入数值型样例数据\n",
    "train_x, test_x, train_y, test_y = data_utils.get_x_y_split(test_rate=0.2)\n",
    "\n",
    "# 训练GBDT模型\n",
    "clf = GradientBoostingClassifier(n_estimators=100,\n",
    "                                 learning_rate=0.1,\n",
    "                                 subsample=0.9,\n",
    "                                 max_depth=4,\n",
    "                                 min_samples_leaf=20,\n",
    "                                 random_state=88)\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "auc_score = roc_auc_score(test_y, clf.predict_proba(test_x)[:, 1])\n",
    "print(\"GBDT模型 AUC: \", auc_score)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "GBDT模型 AUC:  0.7826140873015873\n"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55AA5085FDB64FE397F299A429547C74",
    "notebookId": "6515cdbac6bc713b13747e43",
    "runtime": {
     "status": "default",
     "execution_status": null,
     "is_visible": false
    },
    "scrolled": false,
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "trusted": true,
    "collapsed": false,
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "id": "4FC61B519BE24DD590C9913A15FBDA95",
    "scrolled": false,
    "notebookId": "6515cdbac6bc713b13747e43"
   },
   "source": [
    "#import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import bayes_opt as bo\n",
    "import sklearn.model_selection as sk_ms\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from utils import data_utils"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "trusted": true,
    "collapsed": false,
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "id": "16BEDEF83DF24B65ABB0E52615CECDCB",
    "scrolled": false,
    "notebookId": "6515cdbac6bc713b13747e43"
   },
   "source": [
    "# 确定最优树的颗数\n",
    "def xgb_cv(param, x, y, num_boost_round=10000):\n",
    "    dtrain = xgb.DMatrix(x, label=y)\n",
    "    cv_res = xgb.cv(param, dtrain, num_boost_round=num_boost_round, early_stopping_rounds=30)\n",
    "    num_boost_round = cv_res.shape[0]\n",
    "    return num_boost_round"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BBB1E5E8812C451CA74C714A67427770",
    "notebookId": "6515cdbac6bc713b13747e43",
    "collapsed": false,
    "scrolled": false,
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "trusted": true
   },
   "source": [
    "def train_xgb(params, x_train, y_train, x_test=None, y_test=None, num_boost_round=10000, early_stopping_rounds=30, verbose_eval=50):\n",
    "    \"\"\"\n",
    "    训练xgb模型\n",
    "    \"\"\"\n",
    "    dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "    if x_test is None:\n",
    "        num_boost_round = xgb_cv(params, x_train, y_train)\n",
    "        early_stopping_rounds = None\n",
    "        eval_sets = ()\n",
    "    else:\n",
    "        dtest = xgb.DMatrix(x_test, label=y_test)\n",
    "        eval_sets = [(dtest, 'test')]\n",
    "    model = xgb.train(params, dtrain, num_boost_round, evals=eval_sets, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose_eval)\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8C53ACB0C5114B11A4813FEE96D9981A",
    "notebookId": "6515cdbac6bc713b13747e43",
    "collapsed": false,
    "scrolled": false,
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "trusted": true
   },
   "source": [
    "def xgboost_grid_search(params_space, x_train, y_train, x_test=None, y_test=None, num_boost_round=10000):\n",
    "    \"\"\"\n",
    "    网格调参, 确定其他参数\n",
    "    \"\"\"\n",
    "    # 设置训练参数\n",
    "    if x_test is None:\n",
    "        x_train, x_test, y_train, y_test = sk_ms.train_test_split(x_train, y_train, test_size=0.2, random_state=1)\n",
    "    score_list = []\n",
    "    test_params = list(ParameterGrid(params_space))\n",
    "    for params_try in test_params:\n",
    "        params_try['eval_metric'] = \"auc\"\n",
    "        params_try['random_state'] = 1\n",
    "        clf_obj = train_xgb(params_try, x_train, y_train, x_test, y_test, num_boost_round=num_boost_round,\n",
    "                            early_stopping_rounds=30, verbose_eval=0)\n",
    "        score_list.append(roc_auc_score(y_test, clf_obj.predict(xgb.DMatrix(x_test))))\n",
    "    result = pd.DataFrame(dict(zip(score_list, test_params))).T\n",
    "    print(result)\n",
    "    # 取测试集上效果最好的参数组合\n",
    "    params = test_params[np.array(score_list).argmax()]\n",
    "    return params"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "366E676AD576415E80AC308BE481BEBE",
    "notebookId": "6515cdbac6bc713b13747e43",
    "collapsed": false,
    "scrolled": false,
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "trusted": true
   },
   "source": [
    "def xgboost_bayesian_optimization(params_space, x_train, y_train, x_test=None, y_test=None, num_boost_round=10000, nfold=5, init_points=2, n_iter=5, verbose_eval=0, early_stopping_rounds=30):\n",
    "    \"\"\"\n",
    "    贝叶斯调参, 确定其他参数\n",
    "    \"\"\"\n",
    "    # 设置需要调节的参数及效果评价指标\n",
    "    def xgboost_cv_for_bo(eta, gamma, max_depth, min_child_weight,\n",
    "                          subsample, colsample_bytree):\n",
    "        params = {\n",
    "            'eval_metric': 'auc',\n",
    "            'booster': 'gbtree',\n",
    "            'objective': 'binary:logistic',\n",
    "            'eta': eta,\n",
    "            'gamma': gamma,\n",
    "            'max_depth': int(max_depth),\n",
    "            'min_child_weight': int(min_child_weight),\n",
    "            'subsample': subsample,\n",
    "            'colsample_bytree': colsample_bytree,\n",
    "            'seed': 1\n",
    "        }\n",
    "        if x_test is None:\n",
    "            dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "            xgb_cross = xgb.cv(params,\n",
    "                               dtrain,\n",
    "                               nfold=nfold,\n",
    "                               metrics='auc',\n",
    "                               early_stopping_rounds=early_stopping_rounds,\n",
    "                               num_boost_round=num_boost_round)\n",
    "            test_auc = xgb_cross['test-auc-mean'].iloc[-1]\n",
    "        else:\n",
    "            clf_obj = train_xgb(params, x_train, y_train, x_test, y_test, num_boost_round=num_boost_round,\n",
    "                                early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose_eval)\n",
    "            test_auc = roc_auc_score(y_test, clf_obj.predict(xgb.DMatrix(x_test)))\n",
    "        return test_auc\n",
    "\n",
    "    # 指定需要调节参数的取值范围\n",
    "    xgb_bo_obj = bo.BayesianOptimization(xgboost_cv_for_bo, params_space, random_state=1)\n",
    "    xgb_bo_obj.maximize(init_points=init_points, n_iter=n_iter)\n",
    "    best_params = xgb_bo_obj.max['params']\n",
    "    best_params['max_depth'] = int(best_params['max_depth'])\n",
    "    best_params['min_child_weight'] = int(best_params['min_child_weight'])\n",
    "    best_params['eval_metric'] = 'auc'\n",
    "    best_params['booster'] = 'gbtree'\n",
    "    best_params['objective'] = 'binary:logistic'\n",
    "    best_params['seed'] = 1\n",
    "    return best_params\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "69DF7855BC1D42F78BD0528956DAAC4B",
    "notebookId": "6515cdbac6bc713b13747e43",
    "collapsed": false,
    "scrolled": false,
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "trusted": true
   },
   "source": [
    "# 导入数值型样例数据\n",
    "train_x, test_x, train_y, test_y = data_utils.get_x_y_split(test_rate=0.2)\n",
    "\n",
    "# 经验参数\n",
    "exp_params = {\n",
    "    'eval_metric': 'auc',\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'binary:logistic',\n",
    "    'eta': 0.1,\n",
    "    'gamma': 0.01,\n",
    "    'max_depth': 4,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    'seed': 1\n",
    "}\n",
    "final_xgb_model = train_xgb(exp_params, train_x, train_y, test_x, test_y)\n",
    "auc_score = roc_auc_score(test_y, final_xgb_model.predict(xgb.DMatrix(test_x)))\n",
    "print(\"经验参数模型AUC: \", auc_score)\n",
    "\n",
    "# 随机搜索调参\n",
    "choose_tuner = 'bayesian'  # bayesian grid_search\n",
    "if choose_tuner == 'grid_search':\n",
    "    params_test = {\n",
    "        'learning_rate': [0.1, 0.15],\n",
    "        'gamma': [0.01, 0],\n",
    "        'max_depth': [4, 3],\n",
    "        'min_child_weight': [1, 2],\n",
    "        'subsample': [0.95, 1],\n",
    "        'colsample_bytree': [1]\n",
    "    }\n",
    "    optimal_params = xgboost_grid_search(params_test, train_x, train_y, test_x, test_y)\n",
    "elif choose_tuner == 'bayesian':\n",
    "    # 贝叶斯调参\n",
    "    params_test = {'eta': (0.05, 0.2),\n",
    "                   'gamma': (0.005, 0.05),\n",
    "                   'max_depth': (3, 5),\n",
    "                   'min_child_weight': (0, 3),\n",
    "                   'subsample': (0.9, 1.0),\n",
    "                   'colsample_bytree': (0.9, 1.0)}\n",
    "    optimal_params = xgboost_bayesian_optimization(params_test, train_x, train_y, test_x, test_y, init_points=5, n_iter=8)\n",
    "\n",
    "print(\"随机搜索调参最优参数: \", optimal_params)\n",
    "\n",
    "final_xgb_model = train_xgb(optimal_params, train_x, train_y, test_x, test_y)\n",
    "auc_score = roc_auc_score(test_y, final_xgb_model.predict(xgb.DMatrix(test_x)))\n",
    "print(\"随机搜索调参模型AUC: \", auc_score)\n",
    "\n",
    "# Pickle方式保存和读取模型\n",
    "def save_model_as_pkl(model, path):\n",
    "    \"\"\"\n",
    "    保存模型到路径path\n",
    "    :param model: 训练完成的模型\n",
    "    :param path: 保存的目标路径\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(model, f, protocol=2)\n",
    "\n",
    "# 保存模型\n",
    "save_model_as_pkl(final_xgb_model, \"./data/xgb_model.pkl\")\n",
    "\n",
    "# SHAP计算\n",
    "#explainer = shap.TreeExplainer(final_xgb_model)\n",
    "#shap_values = explainer.shap_values(train_x)\n",
    "# SHAP可视化\n",
    "#shap.summary_plot(shap_values, train_x, max_display=5)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0]\ttest-auc:0.71255\n[50]\ttest-auc:0.76736\n[100]\ttest-auc:0.78212\n[150]\ttest-auc:0.79377\n[200]\ttest-auc:0.79836\n[249]\ttest-auc:0.80109\n经验参数模型AUC:  0.8002232142857143\n|   iter    |  target   | colsam... |    eta    |   gamma   | max_depth | min_ch... | subsample |\n-------------------------------------------------------------------------------------------------\n| \u001B[0m 1       \u001B[0m | \u001B[0m 0.8073  \u001B[0m | \u001B[0m 0.9417  \u001B[0m | \u001B[0m 0.158   \u001B[0m | \u001B[0m 0.005005\u001B[0m | \u001B[0m 3.605   \u001B[0m | \u001B[0m 0.4403  \u001B[0m | \u001B[0m 0.9092  \u001B[0m |\n| \u001B[0m 2       \u001B[0m | \u001B[0m 0.8025  \u001B[0m | \u001B[0m 0.9186  \u001B[0m | \u001B[0m 0.1018  \u001B[0m | \u001B[0m 0.02285 \u001B[0m | \u001B[0m 4.078   \u001B[0m | \u001B[0m 1.258   \u001B[0m | \u001B[0m 0.9685  \u001B[0m |\n| \u001B[0m 3       \u001B[0m | \u001B[0m 0.7852  \u001B[0m | \u001B[0m 0.9204  \u001B[0m | \u001B[0m 0.1817  \u001B[0m | \u001B[0m 0.006232\u001B[0m | \u001B[0m 4.341   \u001B[0m | \u001B[0m 1.252   \u001B[0m | \u001B[0m 0.9559  \u001B[0m |\n| \u001B[0m 4       \u001B[0m | \u001B[0m 0.7953  \u001B[0m | \u001B[0m 0.914   \u001B[0m | \u001B[0m 0.07972 \u001B[0m | \u001B[0m 0.04103 \u001B[0m | \u001B[0m 4.937   \u001B[0m | \u001B[0m 0.9403  \u001B[0m | \u001B[0m 0.9692  \u001B[0m |\n| \u001B[0m 5       \u001B[0m | \u001B[0m 0.7836  \u001B[0m | \u001B[0m 0.9876  \u001B[0m | \u001B[0m 0.1842  \u001B[0m | \u001B[0m 0.008827\u001B[0m | \u001B[0m 3.078   \u001B[0m | \u001B[0m 0.5095  \u001B[0m | \u001B[0m 0.9878  \u001B[0m |\n| \u001B[0m 6       \u001B[0m | \u001B[0m 0.7896  \u001B[0m | \u001B[0m 0.9547  \u001B[0m | \u001B[0m 0.1185  \u001B[0m | \u001B[0m 0.0273  \u001B[0m | \u001B[0m 4.762   \u001B[0m | \u001B[0m 0.05136 \u001B[0m | \u001B[0m 0.988   \u001B[0m |\n| \u001B[0m 7       \u001B[0m | \u001B[0m 0.7793  \u001B[0m | \u001B[0m 0.9669  \u001B[0m | \u001B[0m 0.08808 \u001B[0m | \u001B[0m 0.02523 \u001B[0m | \u001B[0m 4.443   \u001B[0m | \u001B[0m 2.143   \u001B[0m | \u001B[0m 0.9878  \u001B[0m |\n| \u001B[0m 8       \u001B[0m | \u001B[0m 0.7884  \u001B[0m | \u001B[0m 0.9905  \u001B[0m | \u001B[0m 0.07415 \u001B[0m | \u001B[0m 0.04062 \u001B[0m | \u001B[0m 4.012   \u001B[0m | \u001B[0m 2.498   \u001B[0m | \u001B[0m 0.9274  \u001B[0m |\n| \u001B[0m 9       \u001B[0m | \u001B[0m 0.7765  \u001B[0m | \u001B[0m 0.9931  \u001B[0m | \u001B[0m 0.1572  \u001B[0m | \u001B[0m 0.03451 \u001B[0m | \u001B[0m 4.953   \u001B[0m | \u001B[0m 2.889   \u001B[0m | \u001B[0m 0.9302  \u001B[0m |\n| \u001B[0m 10      \u001B[0m | \u001B[0m 0.7814  \u001B[0m | \u001B[0m 0.9311  \u001B[0m | \u001B[0m 0.09404 \u001B[0m | \u001B[0m 0.04976 \u001B[0m | \u001B[0m 4.207   \u001B[0m | \u001B[0m 2.132   \u001B[0m | \u001B[0m 0.9107  \u001B[0m |\n| \u001B[0m 11      \u001B[0m | \u001B[0m 0.7897  \u001B[0m | \u001B[0m 0.9314  \u001B[0m | \u001B[0m 0.151   \u001B[0m | \u001B[0m 0.005   \u001B[0m | \u001B[0m 3.725   \u001B[0m | \u001B[0m 0.4391  \u001B[0m | \u001B[0m 0.9     \u001B[0m |\n| \u001B[0m 12      \u001B[0m | \u001B[0m 0.7977  \u001B[0m | \u001B[0m 0.9389  \u001B[0m | \u001B[0m 0.1541  \u001B[0m | \u001B[0m 0.005258\u001B[0m | \u001B[0m 3.586   \u001B[0m | \u001B[0m 0.4742  \u001B[0m | \u001B[0m 0.9152  \u001B[0m |\n| \u001B[0m 13      \u001B[0m | \u001B[0m 0.7986  \u001B[0m | \u001B[0m 0.9477  \u001B[0m | \u001B[0m 0.164   \u001B[0m | \u001B[0m 0.005   \u001B[0m | \u001B[0m 3.584   \u001B[0m | \u001B[0m 0.4088  \u001B[0m | \u001B[0m 0.9067  \u001B[0m |\n=================================================================================================\n随机搜索调参最优参数:  {'colsample_bytree': 0.9417022004702574, 'eta': 0.15804867401632372, 'gamma': 0.00500514686678052, 'max_depth': 3, 'min_child_weight': 0, 'subsample': 0.9092338594768798, 'eval_metric': 'auc', 'booster': 'gbtree', 'objective': 'binary:logistic', 'seed': 1}\n[0]\ttest-auc:0.64292\n[50]\ttest-auc:0.79588\n[100]\ttest-auc:0.80990\n[138]\ttest-auc:0.80729\n随机搜索调参模型AUC:  0.8072916666666666\n"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5F10BEDF445548E2938317DD525C964C",
    "notebookId": "6515cdbac6bc713b13747e43",
    "runtime": {
     "status": "default",
     "execution_status": null,
     "is_visible": false
    },
    "scrolled": false,
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2C7AD2BB1DF14248B648D6B96F871231",
    "notebookId": "6515cdbac6bc713b13747e43",
    "collapsed": false,
    "scrolled": false,
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "trusted": true
   },
   "source": [
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from utils import data_utils\n",
    "\n",
    "# 导入数值型样例数据\n",
    "train_x, test_x, train_y, test_y = data_utils.get_x_y_split(test_rate=0.2)\n",
    "\n",
    "clf = lgb.LGBMClassifier(objective='binary',\n",
    "                         boosting_type='gbdt',\n",
    "                         max_depth=3,\n",
    "                         n_estimators=1000,\n",
    "                         subsample=1,\n",
    "                         colsample_bytree=1,\n",
    "                         n_jobs=4)\n",
    "callbacks = [early_stopping(stopping_rounds=30)]\n",
    "\n",
    "lgb_model = clf.fit(train_x, train_y, eval_set=[(test_x, test_y)], eval_metric='auc', callbacks=callbacks)\n",
    "\n",
    "auc_score = roc_auc_score(test_y, lgb_model.predict_proba(test_x)[:, 1])\n",
    "print(\"LightGBM模型 AUC: \", auc_score)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[LightGBM] [Info] Number of positive: 244, number of negative: 556\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000188 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 396\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.305000 -> initscore=-0.823600\n[LightGBM] [Info] Start training from score -0.823600\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nTraining until validation scores don't improve for 30 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[95]\tvalid_0's auc: 0.800223\tvalid_0's binary_logloss: 0.481291\nLightGBM模型 AUC:  0.8002232142857142\n"
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "file_extension": ".py",
   "version": "3.5.2",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
